{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e8f0e5-66e5-48fb-b27c-b459258f7c84",
   "metadata": {},
   "source": [
    "# BeautifulSoup Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1448e-7fd3-4ed2-9868-12c3e3649126",
   "metadata": {},
   "source": [
    "## Basic exos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c231b79-78a9-4d21-b383-ad2c9cc4d6b7",
   "metadata": {},
   "source": [
    "we will master beautifulsoup with concrete exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b08dfdd-4638-42ea-8c50-08374c6e3569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead604ac-659f-4526-9c00-3e3d3539da64",
   "metadata": {},
   "source": [
    "### 1. Extract the title of a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e36cb253-7d35-41c8-b8a2-8e1146937b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub: Let’s build from here · GitHub\n"
     ]
    }
   ],
   "source": [
    "# create a variable containing the URL of the desired website\n",
    "url = \"https://www.github.com\"\n",
    "\n",
    "# send a request to the website and store the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content of the website using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# extract the title of the website from the parsed HTML\n",
    "title = soup.title.string\n",
    "\n",
    "# print the title of the website\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14a59c-b524-4a6d-9c25-f84a47e93ebd",
   "metadata": {},
   "source": [
    "### 2. Extract all the links from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f1be670-42b8-4911-8f57-b6e6cff8d75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "# for link in links:\n",
    "#     print(link.get(\"href\"))\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e36373b-32b1-4eea-91ce-4d345a8401db",
   "metadata": {},
   "source": [
    "### 3. Extract all the images from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa396d31-7cfc-43b3-adcb-744ee50082b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "images = soup.find_all(\"img\")\n",
    "\n",
    "# for image in images:\n",
    "#     print(image.get(\"src\"))\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d5b96-4543-42de-9d27-68113adb57bc",
   "metadata": {},
   "source": [
    "### 4. Extract all the text from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1e5acc1-813f-4b90-ae26-6e2ef4c2bc79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10610\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "text = soup.get_text()\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef77db0-bc60-46dc-b50b-5df59c6a7213",
   "metadata": {},
   "source": [
    "### 5. Extract the first paragraph of a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63b1d2e3-7db5-4c1a-9987-c9a00d6511db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harnessed for productivity. Designed for collaboration. Celebrated for built-in security. Welcome to the platform developers love.\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "paragraph = soup.p.string\n",
    "\n",
    "print(paragraph.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0159e04d-4232-49b3-967d-fcc645a3fd3c",
   "metadata": {},
   "source": [
    "### 6. Extract all the headers from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f3e1bec-da35-4840-be31-df94acaa1e22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "headers = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "\n",
    "# for header in headers:\n",
    "#     print(header.string)\n",
    "print(len(headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a28eaa-181e-4257-ad7a-71fea3dd6d7e",
   "metadata": {},
   "source": [
    "### 7. Extract all the tables from a webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a0e6b6a-792b-49cc-85e7-5f5012999fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "# for table in tables:\n",
    "#     print(table)\n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab944e9-9b10-49ab-8334-9b3d0fbc9f6e",
   "metadata": {},
   "source": [
    "### 8. Extract all the list items from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2066471c-7382-4e51-b091-b3bb2236ef98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "lists = soup.find_all(\"li\")\n",
    "\n",
    "# for li in lists:\n",
    "#     if li.string is not None:\n",
    "#         print(li.string.strip())\n",
    "\n",
    "print(len(lists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e6188f-d925-4422-8b9f-683c370fdcda",
   "metadata": {},
   "source": [
    "### 9. Extract all the divs with a specific class from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24f7d43e-acef-4a50-85ee-3e3c710595af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "divs = soup.find_all(\"div\", class_=\"d-none\")\n",
    "\n",
    "# for div in divs:\n",
    "#     print(div)\n",
    "print(len(divs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba383828-c7c9-48cb-b205-6966e006a273",
   "metadata": {},
   "source": [
    "### 10. Extract all the paragraphs with a specific attribute from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fbe66870-a91f-4bd3-94d3-26655facecc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "paragraphs = soup.find_all(\"p\", attrs={\"class\": \"f1-mktg\"})\n",
    "# for p in paragraphs:\n",
    "#     print(p.string)\n",
    "print(len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "748d13ac-6681-4479-b38f-29e649db1d29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company                       \tContact                       \tCountry                       \n",
      "                **************\t                **************\t                **************\n",
      "Alfreds Futterkiste           \tMaria Anders                  \tGermany                       \n",
      "Centro comercial Moctezuma    \tFrancisco Chang               \tMexico                        \n",
      "Ernst Handel                  \tRoland Mendel                 \tAustria                       \n",
      "Island Trading                \tHelen Bennett                 \tUK                            \n",
      "Laughing Bacchus Winecellars  \tYoshi Tannamuri               \tCanada                        \n",
      "Magazzini Alimentari Riuniti  \tGiovanni Rovelli              \tItaly                         \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# the URL of the webpage containing the table\n",
    "url = 'https://www.w3schools.com/html/html_tables.asp'\n",
    "\n",
    "# make a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# find the table element by its ID attribute\n",
    "table = soup.find(id='customers')\n",
    "\n",
    "# find all rows of the table (excluding the header row)\n",
    "rows = table.find_all('tr')\n",
    "# print the header\n",
    "print('\\t'.join([col.text.strip().ljust(30) for col in rows[0].find_all('th')]))\n",
    "print('\\t'.join(['**************'.rjust(30) for col in rows[0].find_all('th')]))\n",
    "# loop through each row and extract the data from the columns\n",
    "for row in rows[1:]:\n",
    "    # get the column values as a list\n",
    "    columns = row.find_all('td')\n",
    "    # print the values of each column separated by a tab\n",
    "    print('\\t'.join([col.text.strip().ljust(30) for col in columns]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394dd10-80af-4748-908a-2023aa52d8df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
