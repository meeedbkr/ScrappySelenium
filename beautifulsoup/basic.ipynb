{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e8f0e5-66e5-48fb-b27c-b459258f7c84",
   "metadata": {},
   "source": [
    "# BeautifulSoup Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1448e-7fd3-4ed2-9868-12c3e3649126",
   "metadata": {},
   "source": [
    "## basic-level web scraping exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c231b79-78a9-4d21-b383-ad2c9cc4d6b7",
   "metadata": {},
   "source": [
    "we will master beautifulsoup with concrete exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b08dfdd-4638-42ea-8c50-08374c6e3569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead604ac-659f-4526-9c00-3e3d3539da64",
   "metadata": {},
   "source": [
    "### 1. Extract the title of a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e36cb253-7d35-41c8-b8a2-8e1146937b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub: Let’s build from here · GitHub\n"
     ]
    }
   ],
   "source": [
    "# create a variable containing the URL of the desired website\n",
    "url = \"https://www.github.com\"\n",
    "\n",
    "# send a request to the website and store the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content of the website using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# extract the title of the website from the parsed HTML\n",
    "title = soup.title.string\n",
    "\n",
    "# print the title of the website\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14a59c-b524-4a6d-9c25-f84a47e93ebd",
   "metadata": {},
   "source": [
    "### 2. Extract all the links from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f1be670-42b8-4911-8f57-b6e6cff8d75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "# for link in links:\n",
    "#     print(link.get(\"href\"))\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e36373b-32b1-4eea-91ce-4d345a8401db",
   "metadata": {},
   "source": [
    "### 3. Extract all the images from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa396d31-7cfc-43b3-adcb-744ee50082b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "images = soup.find_all(\"img\")\n",
    "\n",
    "# for image in images:\n",
    "#     print(image.get(\"src\"))\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d5b96-4543-42de-9d27-68113adb57bc",
   "metadata": {},
   "source": [
    "### 4. Extract all the text from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1e5acc1-813f-4b90-ae26-6e2ef4c2bc79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10610\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "text = soup.get_text()\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef77db0-bc60-46dc-b50b-5df59c6a7213",
   "metadata": {},
   "source": [
    "### 5. Extract the first paragraph of a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63b1d2e3-7db5-4c1a-9987-c9a00d6511db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harnessed for productivity. Designed for collaboration. Celebrated for built-in security. Welcome to the platform developers love.\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "paragraph = soup.p.string\n",
    "\n",
    "print(paragraph.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0159e04d-4232-49b3-967d-fcc645a3fd3c",
   "metadata": {},
   "source": [
    "### 6. Extract all the headers from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f3e1bec-da35-4840-be31-df94acaa1e22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "headers = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "\n",
    "# for header in headers:\n",
    "#     print(header.string)\n",
    "print(len(headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a28eaa-181e-4257-ad7a-71fea3dd6d7e",
   "metadata": {},
   "source": [
    "### 7. Extract all the tables from a webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a0e6b6a-792b-49cc-85e7-5f5012999fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "# for table in tables:\n",
    "#     print(table)\n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab944e9-9b10-49ab-8334-9b3d0fbc9f6e",
   "metadata": {},
   "source": [
    "### 8. Extract all the list items from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2066471c-7382-4e51-b091-b3bb2236ef98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "lists = soup.find_all(\"li\")\n",
    "\n",
    "# for li in lists:\n",
    "#     if li.string is not None:\n",
    "#         print(li.string.strip())\n",
    "\n",
    "print(len(lists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e6188f-d925-4422-8b9f-683c370fdcda",
   "metadata": {},
   "source": [
    "### 9. Extract all the divs with a specific class from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24f7d43e-acef-4a50-85ee-3e3c710595af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "divs = soup.find_all(\"div\", class_=\"d-none\")\n",
    "\n",
    "# for div in divs:\n",
    "#     print(div)\n",
    "print(len(divs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba383828-c7c9-48cb-b205-6966e006a273",
   "metadata": {},
   "source": [
    "### 10. Extract all the paragraphs with a specific attribute from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fbe66870-a91f-4bd3-94d3-26655facecc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "paragraphs = soup.find_all(\"p\", attrs={\"class\": \"f1-mktg\"})\n",
    "# for p in paragraphs:\n",
    "#     print(p.string)\n",
    "print(len(paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dfef32-7b10-47ce-bb1c-4b56f3240d8d",
   "metadata": {},
   "source": [
    "## intermediate-level web scraping exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d2ba59-b232-4799-8509-a943e5b4f111",
   "metadata": {},
   "source": [
    "### 1. Scraping table data from a webpage using Beautiful Soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "748d13ac-6681-4479-b38f-29e649db1d29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company                       \tContact                       \tCountry                       \n",
      "                **************\t                **************\t                **************\n",
      "Alfreds Futterkiste           \tMaria Anders                  \tGermany                       \n",
      "Centro comercial Moctezuma    \tFrancisco Chang               \tMexico                        \n",
      "Ernst Handel                  \tRoland Mendel                 \tAustria                       \n",
      "Island Trading                \tHelen Bennett                 \tUK                            \n",
      "Laughing Bacchus Winecellars  \tYoshi Tannamuri               \tCanada                        \n",
      "Magazzini Alimentari Riuniti  \tGiovanni Rovelli              \tItaly                         \n"
     ]
    }
   ],
   "source": [
    "# the URL of the webpage containing the table\n",
    "url = 'https://www.w3schools.com/html/html_tables.asp'\n",
    "\n",
    "# make a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# find the table element by its ID attribute\n",
    "table = soup.find(id='customers')\n",
    "\n",
    "# find all rows of the table (excluding the header row)\n",
    "rows = table.find_all('tr')\n",
    "# print the header\n",
    "print('\\t'.join([col.text.strip().ljust(30) for col in rows[0].find_all('th')]))\n",
    "print('\\t'.join(['**************'.rjust(30) for col in rows[0].find_all('th')]))\n",
    "# loop through each row and extract the data from the columns\n",
    "for row in rows[1:]:\n",
    "    # get the column values as a list\n",
    "    columns = row.find_all('td')\n",
    "    # print the values of each column separated by a tab\n",
    "    print('\\t'.join([col.text.strip().ljust(30) for col in columns]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a478c893-c68e-4ec7-b021-cf705fbe13f4",
   "metadata": {},
   "source": [
    "### 2. Scraping data from multiple pages using a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017242fc-e6ca-444b-ae67-772d2f581ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing 1-30 of 90,391\n",
      "Showing 31-60 of 90,391\n"
     ]
    }
   ],
   "source": [
    "# the base URL of the webpage\n",
    "url = 'https://www.goodreads.com/quotes/tag/love?page='\n",
    "\n",
    "# loop through page numbers 1 to 3\n",
    "for page in range(1, 3):\n",
    "    # construct the full URL of the page\n",
    "    page_url = url + str(page)\n",
    "    # make a GET request to the page\n",
    "    response = requests.get(page_url)\n",
    "    # parse the HTML content using Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # find the elements containing the data you want\n",
    "    # and do something with them (e.g., print)\n",
    "    # print(soup.title.string)\n",
    "    s = soup.select('.leftContainer > .mediumText')\n",
    "    print(s[0].text.strip().split('\\n')[1])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7c94505-d767-48aa-95ce-3f48bda70b15",
   "metadata": {
    "tags": []
   },
   "source": [
    "git commit -m\"Exo_lvl2_3 Done\" -m\"Scraping data from nested HTML elements.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373f8348-7cc7-48fa-8d38-0b9d20be3512",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Scraping data from nested HTML elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "587aafca-4a19-4f8e-bb14-4255cb651303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"quote\">\n",
      "<div class=\"quoteDetails\">\n",
      "<a class=\"leftAlignedImage\" href=\"/author/show/3565.Oscar_Wilde\">\n",
      "<img alt=\"Oscar Wilde\" src=\"https://images.gr-assets.com/authors/1673611182p2/3565.jpg\"/>\n",
      "</a>\n",
      "<div class=\"quoteText\">\n",
      "      “Be yourself; everyone else is already taken.”\n",
      "  <br/>  ―\n",
      "  <span class=\"authorOrTitle\">\n",
      "    Oscar Wilde\n",
      "  </span>\n",
      "</div>\n",
      "<div class=\"quoteFooter\">\n",
      "<div class=\"greyText smallText left\">\n",
      "     tags:\n",
      "       <a href=\"/quotes/tag/attributed-no-source\">attributed-no-source</a>,\n",
      "       <a href=\"/quotes/tag/be-yourself\">be-yourself</a>,\n",
      "       <a href=\"/quotes/tag/gilbert-perreira\">gilbert-perreira</a>,\n",
      "       <a href=\"/quotes/tag/honesty\">honesty</a>,\n",
      "       <a href=\"/quotes/tag/inspirational\">inspirational</a>,\n",
      "       <a href=\"/quotes/tag/misattributed-oscar-wilde\">misattributed-oscar-wilde</a>,\n",
      "       <a href=\"/quotes/tag/quote-investigator\">quote-investigator</a>\n",
      "</div>\n",
      "<div class=\"right\">\n",
      "<a class=\"smallText\" href=\"/quotes/19884-be-yourself-everyone-else-is-already-taken\" title=\"View this quote\">167067 likes</a>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"action\">\n",
      "<a class=\"gr-button gr-button--small\" href=\"/user/new\" rel=\"nofollow\">Like</a>\n",
      "</div>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "# the URL of the webpage containing the nested elements\n",
    "url = 'https://www.goodreads.com/quotes'\n",
    "\n",
    "# make a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# find the parent element that contains the nested elements\n",
    "parent = soup.find('div', class_='quote')\n",
    "\n",
    "# find the nested elements within the parent element\n",
    "nested_elements = parent.find_all('div', class_='quoteText')\n",
    "print(sou)\n",
    "# loop through the nested elements and extract the data\n",
    "for nested_element in nested_elements:\n",
    "    # extract the data from the nested element and do something with it\n",
    "    # ...\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f58cc9-9494-44d0-ad10-ba6fd08f4643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
